{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "J023_Questions_Language.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q9xq4eUwNrT",
        "outputId": "ff3c3e05-0d2f-4edd-f9f4-dab0a700c2a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "import os\n",
        "import string\n",
        "import math\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDAsIPFV7lEp"
      },
      "source": [
        "FILE_MATCHES = 1\n",
        "SENTENCE_MATCHES = 1"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aognNb42wdsA"
      },
      "source": [
        "def main():\n",
        "    # Calculate IDF values across files\n",
        "    files = load_files(sys.argv[1])\n",
        "    file_words = {\n",
        "        filename: tokenize(files[filename])\n",
        "        for filename in files\n",
        "    }\n",
        "    file_idfs = compute_idfs(file_words)\n",
        "\n",
        "    # Prompt user for query\n",
        "    query = set(tokenize(input(\"Query: \")))\n",
        "\n",
        "    # Determine top file matches according to TF-IDF\n",
        "    filenames = top_files(query, file_words, file_idfs, n=FILE_MATCHES)\n",
        "\n",
        "    # Extract sentences from top files\n",
        "    sentences = dict()\n",
        "    for filename in filenames:\n",
        "        for passage in files[filename].split(\"\\n\"):\n",
        "            for sentence in nltk.sent_tokenize(passage):\n",
        "                tokens = tokenize(sentence)\n",
        "                if tokens:\n",
        "                    sentences[sentence] = tokens\n",
        "\n",
        "    # Compute IDF values across sentences\n",
        "    idfs = compute_idfs(sentences)\n",
        "\n",
        "    # Determine top sentence matches\n",
        "    matches = top_sentences(query, sentences, idfs, n=SENTENCE_MATCHES)\n",
        "    for match in matches:\n",
        "        print(match)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmDwNbSfwm5L"
      },
      "source": [
        "def load_files(directory):\n",
        "    directory=(r\"/content/corpus\")\n",
        "    file_contents = dict()\n",
        "\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            f = open(os.path.join(root, file), \"r\")\n",
        "            file_contents[file] = f.read()\n",
        "\n",
        "    return file_contents"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GUQQ60Dwsy2"
      },
      "source": [
        "def tokenize(document):\n",
        " \n",
        "    punctuation = string.punctuation\n",
        "    stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "    words = nltk.word_tokenize(document.lower())\n",
        "    words = [word for word in words if word not in punctuation and word not in stop_words]\n",
        "\n",
        "    return words\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ildGLlu4wvxG"
      },
      "source": [
        "def compute_idfs(documents):\n",
        "    \n",
        "    idfs = dict()\n",
        "    total_num_documents = len(documents)\n",
        "    words = set(word for sublist in documents.values() for word in sublist)\n",
        "    \n",
        "    for word in words:\n",
        "        num_documents_containing_word = 0\n",
        "        \n",
        "        for document in documents.values():\n",
        "            if word in document:\n",
        "                num_documents_containing_word += 1\n",
        "        \n",
        "        idf = math.log(total_num_documents / num_documents_containing_word)\n",
        "        idfs[word] = idf\n",
        "\n",
        "    return idfs"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHoUYLmEwxX6"
      },
      "source": [
        "def top_files(query, files, idfs, n):\n",
        "  \n",
        "    file_scores = dict()\n",
        "\n",
        "    for file, words in files.items():\n",
        "        total_tf_idf = 0\n",
        "        for word in query:\n",
        "            total_tf_idf += words.count(word) * idfs[word]\n",
        "        file_scores[file] = total_tf_idf\n",
        "\n",
        "    ranked_files = sorted(file_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ranked_files = [x[0] for x in ranked_files]\n",
        "\n",
        "    return ranked_files[:n]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txYHgKepw02x"
      },
      "source": [
        "def top_sentences(query, sentences, idfs, n):\n",
        "  \n",
        "    sentence_scores = dict()\n",
        "\n",
        "    for sentence, words in sentences.items():\n",
        "        words_in_query = query.intersection(words)\n",
        "        \n",
        "        # idf value of sentence\n",
        "        idf = 0\n",
        "        for word in words_in_query:\n",
        "            idf += idfs[word]\n",
        "        \n",
        "        # query term density of sentence\n",
        "        num_words_in_query = sum(map(lambda x: x in words_in_query, words))\n",
        "        query_term_density = num_words_in_query / len(words)\n",
        "\n",
        "        # update sentence scores with idf and query term density values\n",
        "        sentence_scores[sentence] = {'idf': idf, 'qtd': query_term_density}\n",
        "    \n",
        "    # rank sentences by idf then query term density\n",
        "    ranked_sentences = sorted(sentence_scores.items(), key=lambda x: (x[1]['idf'], x[1]['qtd']), reverse=True)\n",
        "    ranked_sentences = [x[0] for x in ranked_sentences]\n",
        "\n",
        "    return ranked_sentences[:n]\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz0jBP_7w4Ua",
        "outputId": "24f0c731-bbc1-44d5-f258-4b1a612bf881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: When was Python 3.0 released?\n",
            "Python 3.0 was released on 3 December 2008.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}